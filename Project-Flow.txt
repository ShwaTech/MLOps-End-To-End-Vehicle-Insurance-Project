1. Create project template by executing template.py file
2. Write the code on setup.py and pyproject.toml file to import local packages
   >> Find more about "setup.py and pyproject.toml" at crashcourse.txt
3. Create a virtual env, activate it and install the requirements from requirements.txt
   conda create -n vehicle python=3.10 -y
   conda activate vehicle
   add required modules to requirements.txt
   Do "pip install -r requirements.txt"
4. Do a "pip list" on terminal to make sure you have local packages installed.



----------------------------------------------- MongoDB Setup -----------------------------------------------

5. Sign up to MongoDB Atlas and create a new project by just providing it a name then next next create.
6. From "Create a cluster" screen, hit "create", Select M0 service keeping other services as default, hit "create deployment"
7. Setup the username and password and then create DB user.
8. Go to "network access" and add ip address - "0.0.0.0/0" so that we can access it from anywhere
9. Go back to project >> "Get Connection String" >> "Drivers" >> {Driver:Python, Version:3.6 or later} 
   >> copy and save the connection string with you(replace password). >> Done.
10. Create folder "notebook" >> do step 11 >>  create file "mongoDB_demo.ipynb" >> select kernal>python kernal>vehicle>>
11. Dataset added to notebook folder
12. Push your data to mongoDB database from your python notebook.
13. Go to mongoDB Atlas >> Database >> browse collection >> see your data in key value format.



-------------------------------------- logging, exception and notebooks --------------------------------------

14. Write the logger file and test it on demo.py
15. Write the exception file and test it on demo.py
16. EDA and Feature Engg notebook added.



----------------------------------------------- Data Ingestion -----------------------------------------------

17. Before we work on "Data Ingestion" component >> Declare variables within constants.__init__.py file >> 
    add code to configuration.mongo_db_connections.py file and define the func for mondodb connection >> 
    Inside "data_access" folder, add code to proj1_data that will use mongo_db_connections.py
    to connect with DB, fetch data in key-val format and transform that to df >>
    add code to entity.config_entity.py file till DataIngestionConfig class >>
    add code to entity.artifact_entity.py file till DataIngestionArtifact class >>
    add code to components.data_ingestion.py file >> add code to training pipeline >> 
    run demo.py (set mongodb connection url first, see next step)
18. To setup the connection url on mac(also work for windows), open bash/powershell terminal and run below command:
                        *** For Bash ***
    set: export MONGODB_URL="mongodb+srv://<username>:<password>......"
    check: echo $MONGODB_URL
                        *** For Powershell ***
    set: $env:MONGODB_URL = "mongodb+srv://<username>:<password>......"
    check: echo $env:MONGODB_URL

    To setup the connection url on Windows, open env variable setting option and add a new variable:
    Name: MONGODB_URL, Value = <url>
    Also add "artifact" dir to .gitignore file


---------------------------- Data Validation, Data Transformation & Model Trainer ----------------------------

19. Complete the work on utils.main_utils.py and config.schema.yaml file (add entire info about dataset for data validation step)
20. Now work on the "Data Validation" component the way we did in step 17 for Data Ingestion. (Workflow mentioned below)
21. Now work on the "Data Transformation" component the way we did in above step. (add estimator.py to entity folder)
22. Now work on the "Model Trainer" component the way we did in above step. (add class to estimator.py in entity folder)


---------------------------------- Model Evaluation, Model Pusher To AWS S3 ----------------------------------

23. Before moving to the next component of **Model Evaluation**, some AWS services setup is needed (via Terraform Infrastructure as Code):

* Ensure AWS region is set to **us-east-1**.

* Use Terraform to create an **IAM user** (name: `dev-shwa-mlops-vehicle`) with the following:

  * Attach `AdministratorAccess` policy.
  * Generate an **access key** and output it securely via Terraform outputs (marked `sensitive = true`).

* Add the credentials into your environment variables in .env File:

  ```

  AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID"
  AWS_SECRET_ACCESS_KEY="AWS_SECRET_ACCESS_KEY"
  
  ```

* Add the **access key, secret key, and region name** to `constants.__init__.py`.

* Update `src/configuration/aws_connection.py` for AWS S3 integration.

* Ensure `constants.__init__.py` contains:

  ```python
  MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE: float = 0.02
  MODEL_BUCKET_NAME = "Shwa-Vehicle-Insurance-Model"
  MODEL_PUSHER_S3_KEY = "model-registry"
  ```

* Use Terraform to provision an **S3 bucket**:

  * Name: `"dev-shwa-vehicle-insurance-model"`
  * Region: `us-east-1`
  * General Purpose storage
  * Configure ACLs by disabling `block_public_acls` and `block_public_policy` if public access is needed.

* Inside `src/aws_storage`, add code to handle pulling/pushing models from the S3 bucket.

* In the `entity` directory, implement `s3_estimator.py` to define functions for data/model push & pull from S3.


24. Now we will start our work on "Model Evaluation" and "Model Pusher" component.


---------------------------------------------- FastAPI App & UI ---------------------------------------------

25. Create the code structure of "Prediction Pipeline" and setup your app.py
26. Add "static" and "template" dir to the project.


---------------------------------------- CI/CD Pipeline & Deployment ----------------------------------------

27. Getting started with the **CI/CD process** (via Terraform Infrastructure as Code):

* **Docker setup**

  * Prepare `Dockerfile` and `.dockerignore` in the application root.
  * These files are application-level and stay outside Terraform scope.

* **Circle CI**

  * Setup `.circleci/config.yaml` to define the CI/CD pipeline.
  * Pipeline will authenticate to AWS using IAM credentials provisioned by Terraform.

* **IAM User**

  * Use Terraform to create an IAM user named `shwa-ci-cd-user`.
  * Attach required policies (e.g., `AmazonEC2FullAccess`, `AmazonECS_FullAccess`, `AmazonEC2ContainerRegistryFullAccess`, or scoped-down permissions).
  * Generate an **access key** via Terraform (`aws_iam_access_key`).
  * Output the key & secret as **sensitive outputs**, then use them in GitHub Actions secrets (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`).

* **ECR Repository**

  * Use Terraform to provision an Amazon ECR repository:

    * Name: `dev-shwa-vehicle-ecr`
    * Region: `us-east-1`
  * Output the repository URI for Docker image pushes.

* **EC2 Server**

  * Use Terraform to provision an EC2 instance for deployment:

    * Name: `dev-shwa-vehicle-machine`
    * Region: `us-east-1`
    * AMI: Ubuntu Server 24.04 LTS (free tier eligible if t2.micro; for t2.medium usage, billing applies).
    * Instance Type: `t2.micro`
    * Key Pair: created and managed in Terraform (e.g., `shwa-vehicle`).
    * Security Group: allow inbound `22 (SSH)`, `80 (HTTP)`, `443 (HTTPS)`.
    * Storage: 30GB gp3 (configurable).
  * Terraform outputs the **public IP** or **DNS** for SSH/CI/CD connection.

* **Terraform Outputs**

  * IAM Access Key/Secret (sensitive).
  * ECR repository URI and Key Pair Created.
  * EC2 instance public DNS/IP.

With this Terraform setup, all infrastructure (IAM, ECR, EC2, networking, keys) is **provisioned automatically** and referenced inside the Circle CI workflow for building, pushing, and deploying Docker images.

28. Open EC2 and Install docker in EC2 Machine:
      ## Optinal
        sudo apt-get update -y
        sudo apt-get upgrade
      ## Required (Because Docker is'nt there in our EC2 server - [docker --version])
        curl -fsSL https://get.docker.com -o get-docker.sh
        sudo sh get-docker.sh
        sudo usermod -aG docker ubuntu
        newgrp docker
